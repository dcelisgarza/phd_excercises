\documentclass[12pt, a4paper]{article}
\usepackage{fontspec}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{paralist}
\usepackage[colorlinks=true,
			linkcolor=blue,
			citecolor=blue,
			urlcolor=blue]
			{hyperref}
\usepackage{verbatim}
\usepackage{cleveref}
\usepackage[left=1.00in, right=1.00in, top=1.00in, bottom=1.00in]{geometry}
\usepackage{booktabs, multirow}
\usepackage{lmodern}
\usepackage[square, numbers, sort&compress]{natbib}
\linespread{1.5}
\title{Radiation damage in Tungsten Literature Critique}
\author{Daniel Celis Garza}
\date{\today}
\begin{document}
	\maketitle
	\section{Introduction}\label{s:intro}
		Arguably the biggest problem facing fusion energy research is the lack of fusion relevant data in materials research. The lack of appropriate nuclear fusion facilities for materials research has prevented it from advancing at the same rate as plasma research.
		
		Many of the materials challenges to fusion energy production are shared between approaches. The portability afforded by this fact facilitates materials research in comparison to plasma. In particular, similar first wall and structural materials can be utilised in both cases.
		
		That said, MCF is much further along in readiness level than ICF so most research is centred around it. Two crucial components of which are the first wall and divertor. Both of which are plasma facing, as such they are subjected to large high energy neutron and heat fluxes. There are very few materials which can withstand such extreme conditions. Of those, tungsten is most commonly proposed due to its relatively low activation, high brittle to ductile transition temperature (BDTT) and low sputtering rate.
		
		For all its virtues, tungsten is far from an ideal material to work with. For one, it is very brittle, even past its BDTT. Reactor designs have been informed by this brittleness, but this creates its own problems because suitable structural materials are hard and expensive to manufacture. Another issue are transmutation products, which despite not being extremely active in the grand scheme of things, greatly affect the material's properties. Radiation damage further embrittles tungsten, changes its thermal conductivity due to transmutation products and produces He bubbles within the material. 
		
		There are also problems with the large scale manufacture and transport of tungsten components as the infrastructure and techniques in current use are not easily or practically scalable. The monetary cost is far too high, production rates too slow, and transport too complicated thanks to the components' brittleness and geometries.
		
		Despite all we know of tungsten, there is a serious lack of experimental data for fusion relevant conditions. There are large gaps in our knowledge that must be filled before achieving sustainable fusion. We currently lack: a good, general way of measuring radiation damage (dpa); experimental data for fusion spectrum neutron irradiation for transmutation products and physical damage; operating temperature characterisation of radiation and load damage; experimental verification of damage cascades; coupled transmutation product and damage models; evolution of heat transport, structural and micro structural properties in fusion relevant environments.
		
		In this essay we explore and critique some of the literature in order to identify key problems and attempt to propose ways of minimising them.
	\section{Displacements per atom}\label{s:dpa}
		One of the bigger problems in radiation damage research is the lack of a standardised way of measuring it. The most common unit is displacements per atom (dpa), but what does this actually mean? It is the average number of displacements each atom undergoes as a result of being irradiated. The fundamental measure of this unit is the number of displacements per unit volume per unit time, $R$
		\begin{align}
			R=N\int _{E_{min}}^{E_{max}}\int _{T_{min}}^{T_{max}}\phi (E)\,\sigma (E,T)\,\upsilon (T)\,\mathrm{d}T\,\mathrm{d}E\;,
		\end{align}
		where $N \equiv$~atom number density, $E \equiv$~incoming particle's energy, $T \equiv$~energy transferred in a collision of a particle of energy $E$ and a lattice atom, $\phi (E) \equiv$~energy dependent particle flux, $\sigma (E,T) \equiv$~the cross section for the collision of a particle with energy $E$ resulting in a transfer of energy $T$ to the struck atom, and $\upsilon (T) \equiv$~is the number of displacements per primary knock on atom as a function of transferred energy $T$. The quantity can then be na\"{i}vely multiplied by the total time (or we could use fluence rather than flux) and sample volume. This ignores the fact that $\sigma (E,T)$ and $\upsilon (T)$ will locally change for the damage cascade, but since the bulk volume is much greater than the damage cascades' we assume the functions remain largely unchanged.
		
		In all, this is a good measure in theory. The catch is that generalised analytical expressions for $\sigma (E,T)$ and $\upsilon (T)$ are extremely hard to obtain and depend on the relative orientation of the incident neutron path and crystallographic parameters.
		
		This however, does not mean they cannot discretised and found via Monte Carlo (MC) approaches. Unfortunately the most common software used to do this is SRIM; a closed source, not actively developed software riddled with archaic restrictions, known problems overcounting, designed to simulate ion irradiation. The community is so desperate for an appropriate measure that widely cited papers have been dedicated to correct dpa measurements post calculation in far from rigorous ways.
		
		Adding insult to injury, SRIM has a vast range of options and numerical values that change a simulation's results. Options that often go un- or misreported in the literature and thus negatively affect the results' reproducibility. If the options and values are reported, then there is often no explanation as to why or how they were chosen. There is also often no regard given to the dependence of $\sigma (E,T)$ and $\upsilon (T)$ on the angle between the neutron source and crystallographic planes of the material. All this provides ample opportunity for error propagation and leads to increasingly desperate attempts at ``fixing'' the issue post simulation, or outright ignoring it.
		
		In contrast to mathematics, in science we are bounded by the real world. For all we might like to come up with elegant models and clever implementations, they must be experimentally sound. A big part of scientific rigour is the transparency, reproducibility and falsifiability of our results by the greater scientific community. We must also make sure to take models for what they are and not blindly follow them or try to manually adjust them after they have produced results. For SRIM in particular, it seems that the calculations are carried out and used extensively for intra paper comparisons but are nigh on useless for broader ones---and if they are, then they are taken with a large degree of scepticism.
		
		These problems would not be so daunting if SRIM were an open source project---at least actively developed by a dedicated team---or if there was a widely accepted alternative. As it stands however, dpa is in theory a very good measure, but a terrible one in practice. At this point it would be more useful to report temperature, flux, exposure time and incident angle between neutron beam and crystallographic planes of the sample instead of dpa. Ideally, the development of better alternatives would do much towards solving the current issues with dpa.
	\section{Analogues to neutron irradiation in tungsten transmutation alloys}\label{s:ni}
		Finding suitably energetic neutrons is a problem that plagues every aspect of fusion energy research. Tungsten, being the primary first wall material and effectively the only viable divertor material is particularly affected by this. 
		
		Experimentally, the approach is to use tungsten ions to irradiate tungsten alloys (self-ion implantation) whose composition contains the four major transmutation products osmium, rhenium and tantalum. Helium and hydrogen implantation also play a significant role in degrading tungsten's properties but data is lacking compared to self-ion implantation. Hydrogen and helium can make their way into the material via diffusion from the plasma. Helium can also be produced inside the material as part of transmutation events, leading to the formation of internal bubbles.
		
		Given the lack of suitably energetic neutron sources, this is not a bad idea. Unfortunately, the penetration depth of the ions is only on the order of nanometres. This greatly limits the possible mechanical tests that can be done on the samples, and is in no way representative of real neutron damage that often passes all the way through the material and into the breeder blanket. Luckily, it is possible to carry out nano indentation and atomic probe tomography (APT) analysis. Nano indentation experiments allow us to study property changes such as work hardening and embrittlement resulting from ion irradiation. APT sheds light on the clustering behaviour of transmutation products which often contributes to such effects.
		
		However, most of these irradiation and mechanical testing experiments and measurements have traditionally been carried at low temperatures $< 400\,\textrm{K}$. It is only recently that they have moved on to higher temperatures $573\,\textrm{K} \le T \le 773\,\textrm{K}$. Even so, taking such sensitive instruments to such high temperatures is a big risk often not worth taking. Worse still is the fact that operating temperatures for tungsten components are over $1000\,\textrm{K}$. Consequently, there is no information available on the clustering behaviour observed in transmutation alloys at such temperatures and how this affects the mechanical properties of tungsten while in operation. The non-linear nature of diffusion, particularly in solids, makes modelling irradiation induced clustering an extremely difficult problem.
		
		The other issue with this approach is the fact that tungsten in operation will be constantly undergoing transmutations and damage from neutron irradiation. Despite there being models for calculating the relative quantities of transmutation products in a sample at any given time, the research has not focused on testing alloys with such compositions. Granted, the models are not perfect (see \cref{s:tm}) but they provide better approximations to real transmutation alloys than what is currently used. In fact, most experimental studies utilise very idealised alloys whose compositions often lack components and/or have unrealistic integer percentage proportions of them. We are still at the stage where we don't yet fully know how these alloys behave, so there is still a need to find general trends or rules that govern their behaviour before we can jump to studying more accurate but harder to analyse alloy compositions.
	\section{Transmutation models}\label{s:tm}
		Transmutation products are arguably the biggest source of problems for tungsten in fusion applications. Not only do they embrittle the material further, they reduce its thermal conductivity. This negatively impact the divertors ability to extract heat from the reactor and causes thermal stress due to the generation of hot spots that cannot be dissipated. As a result, understanding the mechanical and thermal behaviour of transmutation alloys is crucial for moving forward. However, before doing so we need to know their composition. Furthermore, the composition changes with time in a non-trivial way, and thus so will the mechanical properties. Worse still are the very long timescales in the order of years or tens of years over which this happens. Even if we were able to expermentally irradiate tungsten with appropriately energetic neutron it would take years before we could characterise their behaviour, and doing so would be a problematic due to radioactive decay.
		
		The way we go about solving this issue is to model it. CCFE's FISPACT is perhaps the golden standard for transmutation product calculation. The software takes an MC approach at calculating transmutation and decay products of a sample given certain conditions.
		
		The code utilises external data provided by the European Activation File which provide cross sections and decay rates for a wide range of isotopes. Unfortunately there is a lack cross section data for certain neutron energies which contribute in a non-negligible manner to a fusion environment. Interpolating to fill the gaps would not be appropriate as the data is non-smooth and subject to resonance peaks.
		
		In general terms the cross section data is then multiplied by the neutron spectrum and normalised by dividing by the product's integral over the energy range. However, this would require us to have analytical expressions for both quantities and would also make drawing appropriately distributed neutrons via Markov-Chain Monte Carlo algorithms a very slow process. So code ``zooms'' into the biggest peaks and approximates them with Gaussian distributions, thereby ignoring everything else. While this does make things easier to implement, it loses much of the detail and in particular fails to capture peaks' asymmetry where skew normal distributions would provide a much better description, be relatively simple to implement, and would not significantly impact computational time.
		
		Self shielding effects lowers neutron energies into large areas where absorption cross sections peak due to resonances, particularly for heavy elements such as tungsten. There are neutron transport codes which can accurately model these at the expense of being very computationally expensive and having being limited to finding group averaged reaction cross sections. FISPACT uses results from such codes to correct its less accurate but faster binning approach. There are however some potential sources for error here, the corrections themselves, the source and target geometries used in the Monte Carlo N-Particle Transport Code (MCNP), and the accuracy of such predictions. Of particular concern are the source and target geometries (spherical in this case) as they do not consider crystallographic effects and are not representative of actual geometries. Furthermore, the group averaged cross sections are strongly dependent on depth but the correction factors did not vary much, and as such were averaged into a single value. The problem with all this is that the results are likely to be accurate only at certain depths, geometries, and relative source and target positions.
		
		Another huge aspect that puts into question the accuracy of the simulations---but not the code---is that all cross sections were obtained around room temperature $\sim 293.6\,\textrm{K}$ and real irradiation events will happen at much higher temperatures.
		
		Despite all its issues, FISPACT gives us a good idea of how materials evolve with time as they are irradiated with fusion spectrum neutrons. Consequently, it gives us a good---if flawed---idea of how to design tungsten alloys with more accurate amounts of transmutation products at any given point in the lifetime of plasma facing components than current, idealised, compositions.
	\section{Micromechanics to macromechanics}\label{s:mm}
		Working with radioactive samples inhibits our ability to study them in bulk, limiting us to micro and nano scale sizes. This would be fine if the materials we're interested in were perfect crystals, as a small sample would be representative of the whole. Furthermore, from a scientific perspective, micro and nano scale tests provide crucial characterisation information, thus providing us with a better understanding of fundamental properties. In contrast, engineering applications have vastly different requirements. For instance, a material's micro structure plays a big role in determining its bulk characteristics and therefore its performance. Bulk samples have sizes on the order of millions of grains, whereas micro and nano samples' sizes are on the order of a few grains.
		
		One of the most often used techniques for probing property changes is nano indentation. Which is capable of probing the plastic and elastic behaviour of nano sized indentations on a sample. This makes it very appropriate for testing ion and neutron irradiated tungsten and tungsten transmutation alloys. As mentioned in \cref{s:ni}, ion irradiation penetrates the material only on the order of nanometres so the tests can be appropriately limited to the irradiated region which can be compared to unirradiated ones. Unfortunately ion irradiation is not representative of neutron irradiation. For neutron irradiated samples, it enables samples to be small enough so as to not pose serious health risks. However the shallow depths, often makes it very difficult to observe property changes without a large degree of statistical uncertainty.
		
		Another problem with this technique is the high susceptibility of measurements to change between different samples. This is a consequence of the fact that it only probes a small, homogeneous regions. Scientifically speaking this is very sound as it reduces noise. Consequently the results depend on the crystallographic plane and the angle at which it is probed. If those parameters are not controlled it will lead to different elastic and plastic behaviour as a result of the activation of different slip systems. Related to this is the fact that geometric effects also play a role in such small scales. The probe's tip produces different stresses depending on its geometry, such differences are significant at small scales and thus requires standardisation. The sample's geometry is another important aspect to be considered, especially an indent's proximity to other indents and the sample edges. Indents are spaced such that the influence zone of the current indent does not approach either the plastic zone of previous indents or the sample edges. However the influence and plastic zones are experimentally derived parameters that depend on temperature, sample geometry and microstructure. Even after controlling for all this, the results do not translate well to engineering applications for reasons previously mentioned.
		
		Another common experimental technique are micro cantilever tests. They suffer from much of the same scale related issues as nano indentations. They can be single crystal or a may contain small number of grains. With micro cantilevers we can characterise dislocation motion, which is of much interest in nuclear fusion energy production. However, such experiments vary in usefulness depending on whether the goal is to characterise elastic or plastic behaviour. Elasticity is largely local, so micro cantilevers are an appropriate way of measuring it. However, plasticity is not such a localised phenomenon and is therefore largely dependent on size at such scales.
				
		A further, glaring fault in nano indentation and micro cantilever measurements is the temperature at which they have been carried out until now. Often not venturing much beyond $\sim 773\, \textrm{K}$, such temperatures are a far cry from the $>1000 \, \textrm{K}$ operating temperatures of tungsten components. Due to the large degree of non-linearity of plastic and elastic behaviours with respect to temperature, attempting to make generalised and accurate predictions is still an open problem. Furthermore, higher temperatures also means a higher rate of annealing defects out of the material. The relationship between simultaneous indentation and annealing is extremely complicated. On one hand, the indentation creates an influence sphere around the plastic zone of the deformation; on the other, higher atomic mobility reduces the size of other static plastic zones (from other indentations or defects) as a function of time while facilitating the indentation process, and reducing the magnitude of edge effects.
	\section{Modelling radiation damage}
		The lack of suitable data for neutron radiation damage means there is a large need for models to be developed. Damage models have four length scales of which only density functional theory (DFT), molecular dynamics (MD) and dislocation dynamics (DD) are of use in fusion energy research. However, each has its own set of problems that are often ignored by the community at large.
		
		DFT is the most fundamental of the three, and is used to acquire potentials and electronic structure information of point defects. However, the approximations done in DFT can lead calculations astray. Considerations such as the choice of basis set, electron correlation vs no electron correlation, diffuse and polarisation functions, and functional of choice all affect the results in a non-trivial way. Indeed the choices made must be motivated by the atoms to be considered, the type of problem to be tackled and even the quantity of interest. There are many ways of doing and/or interpreting a DFT calculation wrong. Due to the N-dimensional nature of atomistic problems, it is also easy for the code to fall into deep local minima and never reach the global minimum. This is especially egregious in potential energy surface (used in molecular dynamics) calculations and slightly less so in geometry optimisations---which impact electronic structure calculations. In particular tungsten and its transmutation products, with their high Z nuclei, limited applicability, and strong covalent nature of their bonds makes designing and choosing proper parameters for DFT very challenging. It is also worth noting that DFT assumes $0\,K$ and while there are ways of correcting such values they are not rigorous and therefore not from first principles as it is often touted.
		
		MD has its own problems in that they often take potential energy surfaces and other values from DFT calculations. However, if those values are accurate then the models are not limited by this. However, their ability to model large scale systems is limited by the their large memory and computational requirements. With the correct considerations they are capable of modelling damage (MCNP) and even dislocation generation and movement. They also---in theory---have the capability of modelling transmutation events during the run. Unfortunately most scientific software is not easily expandable outside of its original scope due to the coding techniques used, so that remains to be done. Temperature can also be modelled with modified Maxwell-Boltzman distributions, assuming solids behave like very restricted gases.
		
		DD is the smallest scale modelling that can ``quickly'' generate macroscopic effects when coupled to the largest scale modelling, finite element methods. This also enables the calculation of long range dislocation-dislocation, dislocation-impurity and dislocation-grain boundary interactions. However, it relies on having an initial dislocation source, movement rules for the material's crystallography, and other parameters such as plastic modulii. However, DD can be the bridge that closes the gap between micromechanics and macromechanics, at least from the theory/simulation side. They do however, depend on many empirical quantities and quantities obtained from more fundamental models---many of which are temperature dependent.
	\section{Conclusion}
	\bibliographystyle{unsrtnat}	
	\bibliography{wlitrev}
\end{document}