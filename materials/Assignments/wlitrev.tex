\documentclass[12pt, a4paper]{article}
\usepackage{fontspec}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{paralist}
\usepackage[colorlinks=true,
			linkcolor=blue,
			citecolor=blue,
			urlcolor=blue]
			{hyperref}
\usepackage{verbatim}
\usepackage{cleveref}
\usepackage[left=1.00in, right=1.00in, top=1.00in, bottom=1.00in]{geometry}
\usepackage{booktabs, multirow}
\usepackage{lmodern}
\usepackage[square, numbers, sort&compress]{natbib}
\linespread{1.5}
\title{Radiation damage in Tungsten Literature Critique}
\author{Daniel Celis Garza}
\date{\today}
\newcommand{\ic}{ICF}
\newcommand{\mc}{MCF}
\newcommand{\bd}{BDTT}
\newcommand{\dm}{DEMO}
\newcommand{\da}{dpa}
\newcommand{\fs}{fusion}
\newcommand{\rl}{relevant}
\newcommand{\en}{energy}
\newcommand{\pr}{production}
\newcommand{\tn}{tungsten}
\newcommand{\n}{neutron}
\newcommand{\ir}{irradiation}
\newcommand{\tr}{transmutation}
\begin{document}
	\maketitle
	\section{Introduction}\label{s:intro}
		Arguably the biggest problem facing \fs{} \en{} research is the lack of \fs{}-\rl{} data in materials research. The lack of appropriate nuclear fusion facilities for materials research has prevented it from advancing at the same rate as plasma research.
		
		Many of the materials challenges to \fs{} \en{} \pr{} are shared between approaches. The portability afforded by this fact facilitates materials research in comparison to plasma. In particular, similar first wall and structural materials can be utilised in both cases.
		
		That said, \mc{} is much further along in readiness level than \ic{} so most research is centred around it. Two crucial components of which are the first wall and divertor. Both of which are plasma-facing, as such they are subjected to large high \en{} \n{} and heat fluxes. There are very few materials which can withstand such extreme conditions. Of those, \tn{} is most commonly proposed due to its relatively low activation, high brittle-ductile transition temperature (\bd{}) and low sputtering rate.
		
		For all its virtues, \tn{} is far from an ideal material to work with. For one, it is very brittle, even past its \bd{}. Reactor designs have been informed by this brittleness, but this creates its own problems because suitable structural materials are hard and expensive to manufacture. Another issue are \tr{} products, which despite not being extremely active in the grand scheme of things, greatly affect the material's properties. Radiation damage further embrittles \tn{}, changes its thermal conductivity due to \tr{} products and produces He bubbles within the material. 
		
		There are also problems with the large-scale manufacture and transport of \tn{} components as the infrastructure and techniques in current use are not easily or practically scalable. The monetary cost is far too high, \pr{} rates too slow, and transport too complicated thanks to the components' brittleness and geometries.
		
		Despite all we know of \tn{}, there is a serious lack of experimental data for \fs{}-\rl{} conditions. There are large gaps in our knowledge that must be filled before achieving sustainable \fs{}. Currently we are lacking a proper way of comparing damage rather than displacements per atom (\da{}); experimental data for \fs{}-spectrum \n{} \ir{} for \tr{} products and physical damage; systematic characterisation of temperature-dependent radiation damage; experimental verification of damage cascades; coupled \tr{} product and damage models; evolution of heat transport, structural and microstructural properties in \fs{}-\rl{} environments.
		
		In this essay we explore and critique the literature in order to identify key problems and attempt to propose ways of minimising them.
		
		\section{Displacements per atom}\label{s:dpa}
		One of the bigger problems in radiation damage research is the lack of a standardised way of measuring it. The most common unit is displacements per atom (\da{}), but what does this actually mean? It is the average number of displacements each atom undergoes as a result of being irradiated. The fundamental measure of this unit is the number of displacements per unit volume per unit time, $R$
		\begin{align}
			R=N\int _{E_{min}}^{E_{max}}\int _{T_{min}}^{T_{max}}\phi (E)\,\sigma (E,T)\,\upsilon (T)\,\mathrm{d}T\,\mathrm{d}E\;,
		\end{align}
		where $N \equiv$~atom number density, $E \equiv$~incoming particle's \en{}, $T \equiv$~\en{} transferred in a collision of a particle of \en{} $E$ and a lattice atom, $\phi (E) \equiv$~\en{} dependent particle flux, $\sigma (E,T) \equiv$~the cross section for the collision of a particle with \en{} $E$ resulting in a transfer of \en{} $T$ to the struck atom, and $\upsilon (T) \equiv$~is the number of displacements per primary knock-on atom as a function of transferred \en{} $T$. The quantity can then be na\"{i}vely multiplied by the total time (or we could use fluence rather than flux) and sample volume. This ignores the fact that $\sigma (E,T)$ and $\upsilon (T)$ will locally change for the damage cascade, but since the bulk volume is much greater than the damage cascades' we assume the functions remain largely unchanged.
		
		In all, this is a good measure in theory. The catch is that generalised analytical expressions for $\sigma (E,T)$ and $\upsilon (T)$ are extremely hard to obtain and depend on the relative orientation of the incident \n{} path and crystallographic parameters.
		
		This however, does not mean they cannot discretised and found via monte-carlo approaches. Unfortunately the most common software used to do this is SRIM; a closed-source, not actively developed software riddled with archaic restrictions, known problems overcounting, designed to simulate ion irradiation. The community is so desperate for an appropriate measure that widely cited papers have been dedicated to correct \da{} measurements post-calculation in far from rigorous ways.
		
		Adding insult to injury, SRIM has a vast range of options and numerical values that change a simulation's results. Options that often go un- or misreported in the literature and thus negatively affect the results' reproducibility. If the options and values are reported, then there is often no explanation as to why or how they were chosen. There is also often no regard given to the dependence of $\sigma (E,T)$ and $\upsilon (T)$ on the angle between the \n{} source and crystallographic planes of the material. All this provides ample opportunity for error propagation and leads to increasingly desperate attempts at ``fixing'' the issue post-simulation, or outright ignoring it.
		
		In contrast to mathematics, in science we are bounded by the real world. For all we might like to come up with elegant models and clever implementations, they must be experimentally sound. A big part of scientific rigour is the transparency, reproducibility and falsifiability of our results by the greater scientific community. We must also make sure to take models for what they are and not blindly follow them or try to manually adjust them after they have produced results. For SRIM in particular, it seems that the calculations are carried out and used extensively for intra-paper comparisons but are nigh on useless for broader ones---and if they are, then they are taken with a large degree of scepticism.
		
		These problems would not be so daunting if SRIM were an open source project---or at least was in active development by a dedicate team---or if there was an alternative. As it stands however, \da{} is in theory a very good measure, but a terrible one in practice. At this point I think it would be more useful to report temperature, flux, exposure time and incident angle between \n{} beam and crystallographic planes of the sample instead of \da{}. Ideally, the development of better alternatives would do much towards solving the current issues with \da{}.
		
		\section{Analogues to \n{} \ir{}}\label{s:ni}
		Finding suitably energetic \n{s} is a problem that plagues every aspect of \fs{} \en{} research. Tungsten, being the primary first wall material and effectively the only viable divertor material is particularly affected by this. 
		
		Experimentally, the approach is to use tungsten ions to irradiate tungsten alloys (self-ion implantation) whose composition contains the four major \tr{} products osmium, rhenium and tantalum. Helium and hydrogen implantation also play a significant role in degrading tungsten's properties but data is lacking compared to self-ion implantation. Hydrogen and helium can make their way into the material via diffusion from the plasma. Helium can also be produced inside the material as part of \tr{} events, leading to the formation of internal bubbles. 
		
		Given the lack of suitably energetic \n{} sources, this is not a bad idea. Unfortunately, the penetration depth of the ions is only a few nm. This greatly limits the possible mechanical tests that can be done on the samples. Luckily, it is possible to carry out nano-indentation and APT analysis to show the clustering and thus embrittlement effects that irradiation and transmutation can have.
		
		However, most of these irradiation and mechanical testing experiments and measurements have traditionally been carried at low temperatures $< 400\,\textrm{K}$. It is only recently that they have moved on to higher temperatures $573\,\textrm{K} \le T \le 773\,\textrm{K}$. Even so, taking such sensitive instruments to such high temperatures is a big risk often not worth taking. Worse still is the fact that operating temperatures for the first wall are over $1000\,\textrm{K}$. Consequently, there is no information available on the clustering behaviour observed in W-Re-Os alloys at such temperatures and how this affects the mechanical properties of tungsten while in operation.
		
		The other issue with this approach is the fact that tungsten in operation will be constantly undergoing transmutations and undergoing constant damage from neutron irradiation. Despite there being models for calculating the relative quantities of transmutation products in a sample at any given time, the research has not focused on testing alloys with such realistic compositions. In fact, most studies focus on observing the behaviour of alloys with unrealistic proportions of transmutation products. We are still at the stage where we don't yet have a lot of knowledge of how these alloys behave, so there is still a need to find general trends or rules that govern our observations before we can jump to studying more accurate but harder to rationalise alloy compositions.
	\begin{comment}	
	\begin{itemize}
		\item \cite{abernethy2016predicting}
		\begin{itemize}
			\item Fabrication of structural divertor components lacks study.
			\item 14.1\;MeV neutron sources lacking, need to study damage cascades.
			\item Molecular dynamics models damage cascades, but do not take many things into account such as transmutations.
			\item DPA is a flawed measure as vacancies and interstitials can annihilate.
			\item DFT is used for the electronic structure of vacancies and interstitials. DFT results vary depending on the functional and basis sets used. They also assume 0\;K.
			\item Experiments are not standardised. They are done at varying temperatures and dpa rates.
			\item Damage is highly dependent and non-linear w.r.t. kinetics (dpa rate and temperature), they greatly affect total damage.
			\item It is hard to extrapolate ion-irradiation damage to neutron irradiation as it lacks transmutation and the interactions are not the same (ions present electronic slowing which is non-linear).
			\item Often simulation results are not detailed as much as they should be---parameters missing, corrections to results not fully detailed or properly justified.
		\end{itemize}
	\end{itemize}
	\end{comment}
	\bibliographystyle{unsrtnat}	
	\bibliography{wlitrev}
\end{document}